{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jul  8 19:35:33 2021\n",
    "\n",
    "@author: swis\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import tools\n",
    "\n",
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "    if src:\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"-\" * 10)\n",
    "    if sample_rate:\n",
    "        print(\"Sample Rate:\", sample_rate)\n",
    "    print(\"Shape:\", tuple(waveform.shape))\n",
    "    print(\"Dtype:\", waveform.dtype)\n",
    "    print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "    print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "    print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "    print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "    print()\n",
    "    print(waveform)\n",
    "    print()\n",
    "\n",
    "def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = torchaudio.transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = torchaudio.transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec\n",
    "\n",
    "class Audioset(Dataset):\n",
    "    '''build the audio dataset to retrieve audio samples'''\n",
    "\n",
    "    def __init__(self, root, name_text, relative_aud_dir, labeltype, domaintype):\n",
    "\n",
    "        self.aud_dir_prefix = os.path.join(root, relative_aud_dir)\n",
    "        self.labeltype = labeltype\n",
    "        self.domaintype = domaintype\n",
    "\n",
    "      # -------------------------------------------------------\n",
    "      # standard audio sample: dur = 3s, sr = 16k, one channel\n",
    "      # -------------------------------------------------------\n",
    "        self.duration = 3\n",
    "        self.sample_rate = 16000\n",
    "        self.channel = 1\n",
    "        self.aud_names = []\n",
    "        self.aud_labels = []\n",
    "\n",
    "        namefile = os.path.join(root, name_text)\n",
    "        with open(namefile, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                x,y = line.strip().split(\" \")\n",
    "                self.aud_names.append(x)\n",
    "                self.aud_labels.append(y)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Number of items in dataset\n",
    "    # ----------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.aud_labels)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Get i'th item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Absolute file path of the ith audio file - concatenate aud_dir with aud_names[i]\n",
    "        aud_dir = os.path.join(self.aud_dir_prefix, self.aud_names[idx])\n",
    "\n",
    "        # Get the Class ID\n",
    "        emo = int(self.aud_labels[idx])\n",
    "\n",
    "        def label_2_cross_entrophy_class(emo, label_list):\n",
    "            for idx, label in enumerate(label_list):\n",
    "                if emo == label:\n",
    "                    return idx\n",
    "\n",
    "        emo = label_2_cross_entrophy_class(emo, self.labeltype)\n",
    "        label = torch.tensor(emo, dtype = torch.long)#nn.CrossEntropyLoss expects its label input to be of type torch.Long\n",
    "\n",
    "        aud = torchaudio.load(aud_dir)\n",
    "        waveform, old_sample_rate = aud[0], aud[1]\n",
    "        #check\n",
    "        print(\"load audio:\")\n",
    "        tools.plot_waveform(waveform,old_sample_rate,title=\"original waveform\")\n",
    "        \n",
    "        waveform = tools.resample(waveform, old_sample_rate, self.sample_rate) \n",
    "        #check\n",
    "        print(\"resample audio:\")\n",
    "        tools.plot_waveform(waveform,self.sample_rate,\"resampled audio\")\n",
    "        \n",
    "        waveform = tools.rechannel(waveform, self.channel)   \n",
    "        #check\n",
    "        print(\"standardize channel to mono:\")\n",
    "        tools.plot_waveform(waveform,self.sample_rate,\"rechanneled audio\")\n",
    "        \n",
    "        waveform = tools.pad_trunc(waveform, self.sample_rate, self.duration)       \n",
    "        #check\n",
    "        print(\"standardize duration to 3s:\")\n",
    "        tools.plot_waveform(waveform,self.sample_rate,\"padded or truncated audio\")\n",
    "\n",
    "        n_fft = 1024\n",
    "        win_length = None\n",
    "        hop_length = 512\n",
    "        spectrogram = T.Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "        )\n",
    "        spec = spectrogram(waveform)\n",
    "        print_stats(spec)\n",
    "        plot_spectrogram(spec[0], title=\"torchaudio\")\n",
    "\n",
    "        mel_spec = tools.mel_spectrogram(waveform)\n",
    "        #check\n",
    "        print(\"Mel Spectrogram of the standardized sample:\")\n",
    "        tools.plt.figure()\n",
    "        tools.plt.imshow(mel_spec.squeeze().numpy())\n",
    "        plot_spectrogram(mel_spec[0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")\n",
    "        \n",
    "        # print(mel_spec.size())\n",
    "        # mel_spec = torch.transpose(mel_spec,1,2)\n",
    "        # print(mel_spec.size())\n",
    "\n",
    "        tools.plt.figure()\n",
    "        tools.plt.imshow(mel_spec.squeeze().numpy())\n",
    "\n",
    "\n",
    "        mel_spec = torchaudio.transforms.AmplitudeToDB(top_db=80)(mel_spec)\n",
    "        print(\"Mel Spectrogram in log scale:\")\n",
    "        # tools.plot_spectrogram(mel_spec[0])\n",
    "        tools.plt.figure()\n",
    "        tools.plt.imshow(mel_spec.squeeze().numpy())\n",
    "\n",
    "        \n",
    "\n",
    "        #do random crop and flip to mel spectrum image, as data augmentation\n",
    "        preprocess = T.Compose([\n",
    "            T.RandomResizedCrop((224, 224)),\n",
    "            # T.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "        if(self.domaintype=='src'):\n",
    "            # resized_mel_spec = preprocess(F.resize(mel_spec, (256, 256))).repeat(3, 1, 1)\n",
    "            resized_mel_spec = preprocess(mel_spec).repeat(3, 1, 1)\n",
    "        elif(self.domaintype=='tar'):\n",
    "            resized_mel_spec = F.resize(mel_spec, (224, 224)).repeat(3, 1, 1)\n",
    "        \n",
    "        # tools.plt.figure()\n",
    "        # tools.plt.imshow(resized_mel_spec[0].squeeze())\n",
    "        #check\n",
    "        # print(\"Mel Spectrogram after data augmentation(random crop and flip), resizing(256 by 256) and channel repeating(3 channels):\")\n",
    "        # tools.plot_spectrogram(resized_mel_spec[0])\n",
    "        # print(\"shape of the model input\",resized_mel_spec.shape)\n",
    "\n",
    "\n",
    "        return resized_mel_spec, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import audioset_test\n",
    "from tools import plot_spectrogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATAROOT ='E:/projects/ser/database'\n",
    "# DATAROOT ='/content/drive/MyDrive/asset/database'\n",
    "\n",
    "em_text = \"emodb2enter.txt\"\n",
    "en_text = \"enter2emodb.txt\"\n",
    "em_file = \"emodb535_raw\"\n",
    "en_file = \"enterface1287_raw\"\n",
    "en_em_list = [1,2,3,5,7]\n",
    "\n",
    "\n",
    "target_data = audioset_test.Audioset(DATAROOT, em_text, em_file, en_em_list,'tar')\n",
    "source_data = audioset_test.Audioset(DATAROOT, en_text, en_file, en_em_list,'src')\n",
    "\n",
    "BATCH_SIZE=1\n",
    "source_loader = DataLoader(source_data, BATCH_SIZE, shuffle=True)\n",
    "target_loader = DataLoader(target_data, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#check\n",
    "print('source :', len(source_data), len(source_loader))\n",
    "print('target :', len(target_data), len(target_loader))  \n",
    "print('Load data complete')\n",
    "\n",
    "for i in range(1):\n",
    "    s_melspec,idx = next(iter(source_loader))\n",
    "    t_melspec,idx = next(iter(target_loader))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a896cb67606cd05151995453dc39b691b955953edd7a5455ff06022f985de96"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
